<!DOCTYPE html>
<html>
<head>

<title>Developing a MapRduce Application</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">

<script type="text/javascript" src="../../js/jquery-1.6.4.min.js"></script>

<link rel="Stylesheet" type="text/css" href="../../js/sh/styles/shCore.css">
<link rel="Stylesheet" type="text/css" href="../../js/sh/styles/shThemeRDark.css">
<script type="text/javascript" src="../../js/sh/scripts/shCore.js"></script>
<script type="text/javascript" src="../../js/sh/scripts/shAutoloader.js"></script>

<script type="text/javascript" src="../../js/main.js"></script>
<script type="text/javascript" src="../../js/ASCIIMathML.js"></script>

<link rel="Stylesheet" type="text/css" href="../../style.css">
<link rel="Stylesheet" type="text/css" href="../../css/main.css">

</head>
<body>
	<div class="hidden">
		<input id="root_path" type="text" value="../../">
	</div>
	<div id="body-wrapper">
		<div id="container">
			<div id="top">
				<div id="page-title">
					<a href="../../index.html">烂笔头</a>
				</div>
				<ul id="top-nav">
				</ul>
			</div>
			<div id="middle">
				
<div class="toc">
<ul>
<li><a href="#toc_1">Developing a MapRduce Application</a>
<ul>
<li><a href="#toc_1.1">The Configuration API</a>
<li><a href="#toc_1.2">Setting up the Development Environment</a>
<li><a href="#toc_1.3">Writing a Unit Test with MRUnit</a>
<li><a href="#toc_1.4">Running Locally on Test Data</a>
<ul>
<li><a href="#toc_1.4.1">Running a job in a local job runner</a>
</ul>
<li><a href="#toc_1.5">Running on a Cluster</a>
<ul>
<li><a href="#toc_1.5.1">Packaging</a>
</ul>
</ul>
</ul>
</div>

<p>
Mon Jan 21 02:24:57 CST 2013
</p>

<h1 id="toc_1">Developing a MapRduce Application</h1>

<h2 id="toc_1.1">The Configuration API</h2>
<ol>
<li>
Hadoop中的组件都使用Hadoop自身的配置api进行配置，类Configuration的一个实例

<li>
Configuration从xml文件中读取配置，xml文件中是一个简单的name-value的结构文件

<li>
属性的类型并不需要在配置文件中指定，而是在读取时可以指定

<li>
可以加载多个配置文件，并且多个文件可以对同一属性进行配置，后加载的覆盖先前加载的。

<li>
如果先加载的配置文件对属性设置了final为true,后加载的文件就不能覆盖这个属性的定义
<pre class="brush:xml">
    &lt;?xml version="1.0"?&gt;
    &lt;configuration&gt;
        &lt;property&gt;
            &lt;name&gt;color&lt;/name&gt;
            &lt;value&gt;yellow&lt;/value&gt;
            &lt;description&gt;Color&lt;/description&gt;
        &lt;/property&gt;
        &lt;property&gt;
            &lt;name&gt;size&lt;/name&gt;
            &lt;value&gt;10&lt;/value&gt;
            &lt;description&gt;Size&lt;/description&gt;
        &lt;/property&gt;
        &lt;property&gt;
            &lt;name&gt;weight&lt;/name&gt;
            &lt;value&gt;heavy&lt;/value&gt;
            &lt;final&gt;true&lt;/final&gt;
            &lt;description&gt;Weight&lt;/description&gt;
        &lt;/property&gt;
        &lt;property&gt;
            &lt;name&gt;size-weight&lt;/name&gt;
            &lt;value&gt;${size},${weight}&lt;/value&gt;
            &lt;description&gt;Size and weight&lt;/description&gt;
        &lt;/property&gt;
    &lt;/configuration&gt;
</pre>
<pre class="brush:java">
    Configuration conf = new Configuration();
    conf.addResource("configuration-1.xml");
    assertThat(conf.get("color"), is("yellow"));
    assertThat(conf.getInt("size", 0), is(10));
    assertThat(conf.get("breadth", "wide"), is("wide"));
</pre>

</ol>
  
<p>
<strong>变量扩展Variable Expansion:</strong>
</p>
<ol>
<li>
属性配置可以有由其他属性组合成，也可以取system的属性

<li>
system设置的优先级就高于配置文件中的

<ul>
<li>
这样就可以在jvm参数中覆盖属性设置了: <code>-Dproperty=value</code>
<pre class="brush:java">
    conf.get("size-weight");//10,heavy
    
    System.setProperty("size", "14");
    conf.get("size-weight");//14,heavy
</pre>

</ul>
</ol>
    
<h2 id="toc_1.2">Setting up the Development Environment</h2>

<p>
<strong>设置用户:</strong>
</p>
<ol>
<li>
Hadoop在HDFS中作用的用户权限由client端的<code>whoami</code>决定的, 同样组信息由<code>groups</code>决定

<li>
然而，Hadoop中的用户并不等同于客户机中的用户

<li>
可以明确指出要使用Hadoop中要使用的用户和组：

<ul>
<li>
<strong>hadoop.job.ugi</strong>: 以逗号分隔的列表，第一个为用户，后面的为组

</ul>
<li>
通过属性可以设置HDFS页面中所使用的用户：

<ul>
<li>
<strong>dfs.web.ugi</strong>

</ul>
</ol>
  
<p>
<strong>如果启动job中不使用-conf选项，将使用$HADOOP_INSTALL中conf子文件夹中的配置信息</strong>
</p>
<pre  class="brush:bash">
    $ hadoop fs -conf conf/hadoop-local.xml -ls .
</pre>
  
<p>
<strong>GenericOptionParser,Tool,ToolRunner:</strong>
</p>


<h2 id="toc_1.3">Writing a Unit Test with MRUnit</h2>
<p>
<strong>Mapper:</strong>
</p>
<pre  class="brush:java">
    import java.io.IOException;                              
    import org.apache.hadoop.io.*;                           
    import org.apache.hadoop.mrunit.mapreduce.MapDriver;     
    import org.junit.*;                                      
    public class MaxTemperatureMapperTest {
        @Test
            public void processesValidRecord() throws IOException, InterruptedException {
                Text value = new Text("0043011990999991950051518004+68750+023550FM-12+0382" +
                        // Year ^^^^
                        "99999V0203201N00261220001CN9999999N9-00111+99999999999");
                        // Temperature ^^^^^
                new MapDriver&lt;LongWritable, Text, Text, IntWritable&gt;()
                    .withMapper(new MaxTemperatureMapper())
                    .withInputValue(value)
                    .withOutput(new Text("1950"), new IntWritable(-11))
                    .runTest();
            }
    }
</pre>

<p>
<strong>Reducer:</strong>
</p>
<pre  class="brush:java">
       @Test
       public void returnsMaximumIntegerInValues() throws IOException, InterruptedException {
           new ReduceDriver&lt;Text, IntWritable, Text, IntWritable&gt;()
               .withReducer(new MaxTemperatureReducer())
               .withInputKey(new Text("1950"))
               .withInputValues(Arrays.asList(new IntWritable(10), new IntWritable(5)))
               .withOutput(new Text("1950"), new IntWritable(10))
               .runTest();
       }
</pre>
  
<h2 id="toc_1.4">Running Locally on Test Data</h2>
<h3 id="toc_1.4.1">Running a job in a local job runner</h3>
<pre  class="brush:java">
    public class MaxTemperatureDriver extends Configured implements Tool {
    
            @Override
            public int run(String[] args) throws Exception {
                if (args.length != 2) {
                    System.err.printf("Usage: %s [generic options] &lt;input&gt; &lt;output&gt;\n", 
                            getClass().getSimpleName());
                    ToolRunner.printGenericCommandUsage(System.err);
                    return -1;
                }
                
                Job job = new Job(getConf(), "Max temperature");
                job.setJarByClass(getClass());
                
                FileInputFormat.addInputPath(job, new Path(args[0]));
                FileOutputFormat.setOutputPath(job, new Path(args[1]));
                
                job.setMapperClass(MaxTemperatureMapper.class);
                job.setCombinerClass(MaxTemperatureReducer.class);
                job.setReducerClass(MaxTemperatureReducer.class);
                
                //input type 由input format确定，默认是TextInputFormat:(LongWritable , Text)
                
                job.setOutputKeyClass(Text.class);
                job.setOutputValueClass(IntWritable.class);
                
                return job.waitForCompletion(true) ? 0 : 1;
            }
            
            public static void main(String[] args) throws Exception {
                int exitCode = ToolRunner.run(new MaxTemperatureDriver(), args);
                System.exit(exitCode);
            }
    }
</pre>
 
<ol>
<li>
Hadoop提供了一个local job runner, 这是一个简化版的MapReduce执行引擎

<li>
将配置属性<code>mapred.job.tracker</code>(YARN MapReduce中的属性是<code>mapreduce.framework.name</code>)设置为<code>local</code>将激活这个local job runner.job将在进程内运行

</ol>
 
<h2 id="toc_1.5">Running on a Cluster</h2>

<h3 id="toc_1.5.1">Packaging</h3>
<ol>
<li>
在一个分布式环境中，需要将job打包，发往集群中

</ol>

			</div>

			<div id="bottom">
				&copy; 2012 王兴朝
			</div>
		</div>
	<div>
</body>
</html>
