<!DOCTYPE html>
<html>
<head>

<title>Setting up a Hadoop Cluster </title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">

<script type="text/javascript" src="../../js/jquery-1.6.4.min.js"></script>

<link rel="Stylesheet" type="text/css" href="../../js/sh/styles/shCore.css">
<link rel="Stylesheet" type="text/css" href="../../js/sh/styles/shThemeRDark.css">
<script type="text/javascript" src="../../js/sh/scripts/shCore.js"></script>
<script type="text/javascript" src="../../js/sh/scripts/shAutoloader.js"></script>

<script type="text/javascript" src="../../js/main.js"></script>
<script type="text/javascript" src="../../js/ASCIIMathML.js"></script>

<link rel="Stylesheet" type="text/css" href="../../style.css">
<link rel="Stylesheet" type="text/css" href="../../css/main.css">

</head>
<body>
	<div class="hidden">
		<input id="root_path" type="text" value="../../">
	</div>
	<div id="body-wrapper">
		<div id="container">
			<div id="top">
				<div id="page-title">
					<a href="../../index.html">烂笔头</a>
				</div>
				<ul id="top-nav">
				</ul>
			</div>
			<div id="middle">
				

<h1 id="toc_1"> Setting up a Hadoop Cluster</h1>

<p>
当前日期: 02/25/2013 Mon 
</p>

<div class="toc">
<ul>
<li><a href="#toc_1"> Setting up a Hadoop Cluster</a>
<ul>
<li><a href="#toc_1.1">1 集群规范</a>
<ul>
<li><a href="#toc_1.1.1">1.1 网络拓扑</a>
</ul>
<li><a href="#toc_1.2">2 集群的构建与安装</a>
<ul>
<li><a href="#toc_1.2.1">2.1 安装java环境</a>
<li><a href="#toc_1.2.2">2.2 创建Hadoop专用账号</a>
<li><a href="#toc_1.2.3">2.3 安装Hadoop</a>
<li><a href="#toc_1.2.4">2.4 测试已安装的Hadoop</a>
</ul>
<li><a href="#toc_1.3">3 SSH配置</a>
<li><a href="#toc_1.4">4 Hadoop的配置 </a>
<ul>
<li><a href="#toc_1.4.1">4.1 配置管理</a>
<li><a href="#toc_1.4.2">4.2 环境变量设置</a>
<ul>
<li><a href="#toc_1.4.2.1">4.2.1 内存</a>
<li><a href="#toc_1.4.2.2">4.2.2 Java</a>
<li><a href="#toc_1.4.2.3">4.2.3 系统日志</a>
<li><a href="#toc_1.4.2.4">4.2.4 ssh配置</a>
</ul>
</ul>
</ul>
</ul>
</div>

<h2 id="toc_1.1">1 集群规范</h2>
<ul>
<li>
使用商业硬件

<ol>
<li>
不代表低端硬件，故障率太高，会带来很大的维护成本

<li>
不使用昂贵的硬件，性价比太低

</ol>
<li>
关于RAID

<ol>
<li>
Namenode使用Raid1

<li>
Datanode可以使用Raid0提高磁盘效率，但一般不使用，而使用Hadoop自身的JBOD(Just a bunch of disks)

<ul>
<li>
RAID0 有木桶效应，受限于性能差的磁盘

<li>
JBOD循环使用磁盘

<li>
一个测试中JBOD比RADID 0快30%

</ul>
</ol>
<li>
计算集群增长量

<ol>
<li>
1 week = (3T/week*replicationNum)*1.3

<li>
30%用于中间文件和日志

</ol>
</ul>

<h3 id="toc_1.1.1">1.1 网络拓扑</h3>
<p>
<img src="hadoop_topology.png" />
</p>
<ol>
<li>
一般为两层架构

<li>
一个机架为30~40台机器，使用一个千兆交换机

<li>
各机架的交换机连接一个上层的千兆交换机（或更好）

<li>
机架内的node间通信要比机架间的好的多

</ol>

<p>
<strong>机架配置:</strong>
对于多机架的集群，配置node的网络位置（和机架的映射关系），有助于极大的提升hadoop的效率
</p>
<ol>
<li>
接口DNSToSwitchMapping用于解析node和网络位置的对应关系
<pre  class="brush:java">
        public interface DNSToSwitchMapping {
            public List&lt;String&gt; resolve(List&lt;String&gt; names);
            //参数是ip列表
            //返回值是代表网络位置的字符串
        }
</pre>

<li>
属性 <code>topology.node.switch.mapping.impl</code> 指定其要使用的实现类

<ul>
<li>
默认实现是ScriptBasedMapping,

<li>
它使用一个用户指定的脚本来决定映射关系

<li>
脚本通过属性 <code>topology.script.file.name</code> 来指定

</ul>
<li>
使用这部分信息的主要有两块

<ul>
<li>
Namenode用于优化分配Block

<li>
Mapreduce尽量避免机架间传输（jobtracker)

</ul>
</ol>
  

<h2 id="toc_1.2">2 集群的构建与安装</h2>
<h3 id="toc_1.2.1">2.1 安装java环境</h3>
<ul>
<li>
&gt;=jdk1.6 (推荐使用sun jdk)

</ul>
    
<h3 id="toc_1.2.2">2.2 创建Hadoop专用账号</h3>
<ol>
<li>
创建hadoop专用用户，与其他服务隔离

<li>
对于小集群，通常会对用户主目录使用NTFS

<ol>
<li>
实现ssh分布式

<li>
使用autofs, 按需要挂载

<li>
autofs还提供挂载失败时的备用方案

</ol>
</ol>
   
<h3 id="toc_1.2.3">2.3 安装Hadoop</h3>
<ol>
<li>
将Hadoop安装在一个合理的地方

<li>
通常为/usr/local或/opt下

</ol>
 
<h3 id="toc_1.2.4">2.4 测试已安装的Hadoop</h3>

<h2 id="toc_1.3">3 SSH配置</h2>
<ol>
<li>
Hadoop跨集群的控制脚本信赖于ssh

<li>
需要将集群中的机器配成无密码登录

</ol>
 
<h2 id="toc_1.4">4 Hadoop的配置 </h2>
<ol>
<li>
配置文件放在conf目录中

<li>
也可以放在Hadoop安装目录之外的目录中，但启动守护进程时，需要使用-config选项指定配置的位置

</ol>


<table>
<tr>
<th>
Filename
</th>
<th>
Format
</th>
<th>
Description
</th>
</tr>
<tr>
<td>
hadoop-env.sh
</td>
<td>
Bash script
</td>
<td>
Environment variables that are used in the scripts to run Hadoop
</td>
</tr>
<tr>
<td>
core-site.xml
</td>
<td>
Hadoop configuration XML
</td>
<td>
Configuration settings for Hadoop Core, such as I/O settings that are common to HDFS and MapReduce
</td>
</tr>
<tr>
<td>
hdfs-site.xml
</td>
<td>
Hadoop configuration XML
</td>
<td>
Configuration settings for HDFS daemons: the namenode, the secondary namenode, and the datanodes
</td>
</tr>
<tr>
<td>
mapred-site.xml
</td>
<td>
Hadoop configuration XML
</td>
<td>
Configuration settings for MapReduce daemons: the jobtracker, and the tasktrackers
</td>
</tr>
<tr>
<td>
masters
</td>
<td>
Plain text
</td>
<td>
A list of machines (one per line) that each run a secondary namenode
</td>
</tr>
<tr>
<td>
slaves
</td>
<td>
Plain text
</td>
<td>
A list of machines (one per line) that each run a datanode and a task-tracker
</td>
</tr>
<tr>
<td>
hadoop-metrics .properties
</td>
<td>
Java Properties
</td>
<td>
Properties for controlling how metrics are published in Hadoop
</td>
</tr>
<tr>
<td>
log4j.properties
</td>
<td>
Java Properties
</td>
<td>
Properties for system logfiles, the namenode audit log, and the task log for the tasktracker child process
</td>
</tr>
</table>


<h3 id="toc_1.4.1">4.1 配置管理</h3>
<ol>
<li>
Hadoop集群没有统一的配置信息

<li>
而是集群中的每个结点拥有自己的一组配置信息

<li>
需要系统管理员来保证配置文件之间的同步

<li>
当然Hadoop也被设计成整个集群使用一套配置文件,但也有不适用的地方：

<ol>
<li>
集群中机器结点配置不一样时

<li>
可以分机器类

</ol>
</ol>
  
<p>
<strong>控制脚本:</strong> 
</p>
<ol>
<li>
Hadoop提供了控制脚本用于启动或关闭整个集群中的守护进程

<li>
需要告诉Hadoop集群中包含哪些机器

<li>
两个配置文件存储了这部分信息，每一行存储一个hostname和一个ip address

<li>
masters

<ul>
<li>
用于记录运行第二namenode的所有机器

</ul>
<li>
slaves

<ul>
<li>
用于记录所有运行datanode和tasktracker的机器

</ul>
<li>
这两个文件放在配置目录中

<li>
但slaves的位置可在在hadoop.env通过HADOOP_SLAVES另行指定

<li>
这些配置文件无需分发到各工作结点上

<li>
它们只会被运行控制脚本的namenode使用

<li>
start-dfs.sh, 用于启动集群中所有 HDFS的守护进程，并在运行脚本的机器上启动一个namenode

<ol>
<li>
Starts a namenode on the local machine (the machine that the script is run on)

<li>
Starts a datanode on each machine listed in the slaves file

<li>
Starts a secondary namenode on each machine listed in the masters file

</ol>
<li>
start-mapred.sh, 用于启动集群中所有 MapReduce的守护进程

<ol>
<li>
Starts a jobtracker on the local machine

<li>
Starts a tasktracker on each machine listed in the slaves file

</ol>
<li>
MapReduce的控制脚本并不会使用masters配置文件

<li>
最终是通过hadoop-daemon.sh这个脚本完成工作的

</ol>

<p>
<strong>主要结点:</strong> 
</p>
<ol>
<li>
主要结点的守护进程：namenode, secondary namenode, jobtracker

<li>
这些守护进程可以运行于一台机器上，集群较小时

<li>
但随着机器的增大，应该运行于不同的机器上

<li>
不管是否在一台机器上，它的运行遵循下列规则

<ul>
<li>
Run the HDFS control scripts from the namenode machine. The masters file should contain the address of the secondary namenode.

<li>
Run the MapReduce control scripts from the jobtracker machine

</ul>
</ol>

<ol>
<li>
当namenode与jobtracker运行于不同的机器上时，它们的slaves配置文件应该保持同步

</ol>

<h3 id="toc_1.4.2">4.2 环境变量设置</h3>

<h4 id="toc_1.4.2.1">4.2.1 内存</h4>

<ol>
<li>
默认为每个守护进程分配1G内存, 通过 <code>hadoop-env.sh</code> 中的变量 <code>HADOOP_HEAPSIZE</code> 控制

<li>
工作结点内存大小的结点需要考虑到map task和reduce task的，因为这些task是在独立jvm里运行的

<ul>
<li>
一个结点，最多的map task:    mapred.tasktracker.map.tasks.maximum,    默认为2

<li>
一个结点，最多的reduce task: mapred.tasktracker.reduce.tasks.maximum, 默认为2

<li>
一个task的内存：mapred.child.java.opts, 默认为200m

<li>
工作结点内存 = datanode + tasktracker + maptasks + reducetasks = 1G + 1G + 200m*2 + 200m*2 = 2.8G

<li>
经验之谈：task总数与cpu数目之比的1与2之间

<li>
 计算内存，最后不要忘记机器上其他不属于hadoop进程的内存

</ul>
<li>
主结点内存，若运行namenode, secondary namenode, jobtracker, 默认需要3G内存

<li>
计算namenode 进程的内存

<ul>
<li>
namenode内存的大小依赖于集群中文件的总数和块数

<li>
默认1G内存，一般足够几百万文件使用了

<li>
保守使用，通常认为1G内存可用于一百万个文件块分配

<li>
namenode 进程内存 = ( diskSize*datanodeNumber/blockSize/replicationNumber/100000 ) G

<li>
在 <code>hadoop-env.sh</code> 中的可以专门设置namenode内存的大小： <code>HADOOP_NAMENODE_OPTS</code> 

<li>
在 <code>hadoop-env.sh</code> 中的可以专门设置sceondary namenode内存的大小： <code>HADOOP_SECONDARYNAMENODE_OPTS</code>, 应该和namenode保持一致 

</ul>
<li>
在 <code>hadoop-env.sh</code> 中也可以为其他守护进程设置内存大小

</ol>
 
<h4 id="toc_1.4.2.2">4.2.2 Java</h4>
<ol>
<li>
hadoop使用 <code>hadoop-env.sh</code> 的变量 <code>JAVA_HOME</code> 或系统环境变量 <code>JAVA_HOME</code> 来决定使用的jdk

<li>
推荐在 <code>hadoop-env.sh</code> 设置，可以使整个集群保持一致

</ol>
 
<h4 id="toc_1.4.2.3">4.2.3 系统日志</h4>
<ol>
<li>
日志文件默认存放在 <code>$HADOOP_INSTALL/logs</code> 下

<li>
可以通过 <code>hadoop-env.sh</code> 的变量 <code>HADOOP_LOG_DIR</code> 修改

<li>
日志文件夹不存在，会自动创建， <em><strong>但要注意创建权限</strong></em> 

<li>
两类日志文件

<ul>
<li>
.log结尾，由log4j记录，每天rotate, 包含大部分的日志信息

<li>
.out结尾，stdout和stderr的组成，每次重启rotate，最多五个，只有很少的日志内容

<li>
日志文件名 = 用户名-进程名-主机名.(log/out)

<li>
上面的用户名由 <code>hadoop-env.sh</code> 中的 <code>HADOOP_IDENT_STRING</code> 指定

</ul>
</ol>
   
<h4 id="toc_1.4.2.4">4.2.4 ssh配置</h4>
<ol>
<li>
主结点的控制脚本可以通过ssh在所有工作结点上运行命令

<li>
<strong>ConnetionTimeout:</strong> 调整合适的超时大小

<li>
<strong>StrictHostKeyChecking:</strong> 不询问，自动添加远程主机known hosts

<li>
在 <code>hadoop-env.sh</code> 中配置 <code>HADOOP_MASTER</code>

<ul>
<li>
在工作结点守护进程启动时，会把以  <code>HADOOP_MASTER</code> 为根的目录树与本地的 <code>HADOOP_INSTALL</code> 进行同步

<li>
大型集群为了防止启动时主结点rsync压力过大，可以设置 <code>HADOOP_SLAVE_SLEEP</code> 单位为秒，会在调用工作结点两个指令间隙休眠一段时间

</ul>
</ol>

			</div>

			<div id="bottom">
				&copy; 2012 王兴朝
			</div>
		</div>
	<div>
</body>
</html>
