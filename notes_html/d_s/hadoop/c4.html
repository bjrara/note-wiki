<!DOCTYPE html>
<html>
<head>

<title>Hadoop I/O</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">

<script type="text/javascript" src="../../js/jquery-1.6.4.min.js"></script>

<link rel="Stylesheet" type="text/css" href="../../js/sh/styles/shCore.css">
<link rel="Stylesheet" type="text/css" href="../../js/sh/styles/shThemeRDark.css">
<script type="text/javascript" src="../../js/sh/scripts/shCore.js"></script>
<script type="text/javascript" src="../../js/sh/scripts/shAutoloader.js"></script>

<script type="text/javascript" src="../../js/main.js"></script>
<script type="text/javascript" src="../../js/ASCIIMathML.js"></script>

<link rel="Stylesheet" type="text/css" href="../../style.css">
<link rel="Stylesheet" type="text/css" href="../../css/main.css">

</head>
<body>
	<div class="hidden">
		<input id="root_path" type="text" value="../../">
	</div>
	<div id="body-wrapper">
		<div id="container">
			<div id="top">
				<div id="page-title">
					<a href="../../index.html">烂笔头</a>
				</div>
				<ul id="top-nav">
				</ul>
			</div>
			<div id="middle">
				
<div class="toc">
<ul>
<li><a href="#toc_1">Hadoop I/O</a>
<ul>
<li><a href="#toc_1.1">数据完整性 Integrity</a>
<ul>
<li><a href="#toc_1.1.1">HDFS中的数据完整性 </a>
<li><a href="#toc_1.1.2">LocalFileSystem类 </a>
<li><a href="#toc_1.1.3">ChecksumFileSystem类 </a>
</ul>
<li><a href="#toc_1.2">文件压缩File Compression</a>
<ul>
<li><a href="#toc_1.2.1">Codecs</a>
<li><a href="#toc_1.2.2">压缩与输入切片Compression and Input Splitting</a>
<li><a href="#toc_1.2.3">在MapReduce中使用压缩Using Compression in MapReduce</a>
</ul>
<li><a href="#toc_1.3">序列化Serialization</a>
<ul>
<li><a href="#toc_1.3.1">Writable接口</a>
</ul>
</ul>
</ul>
</div>
<h1 id="toc_1">Hadoop I/O</h1>

<h2 id="toc_1.1">数据完整性 Integrity</h2>
<p>
为了保证数据不丢失和损坏，在写入或读出数据时进行校验
</p>

<h3 id="toc_1.1.1">HDFS中的数据完整性 </h3>
<p>
<strong>校验和checksum:</strong>
</p>
<ul>
<li>
HDFS的写入数据时计算较验和，在读取时验证校验和

<li>
HDFS对每io.bytes.per.checksum个（default 512)字节计算checksum

<li>
使用CRC-32, checksum为4bytes

<li>
默认存储checksum的额外开销低于1%

</ul>
 
<p>
<strong>写入时:</strong>
</p>
<ol>
<li>
Datanode在存储它接收到的data和它们的checksum前，进行验证

<li>
这些数据来自client或其他datanode

<li>
当client向pipeline写入数据时:

<ul>
<li>
只有最后一个datanode负责验证checksum

<li>
验证出错时，client会收到一个ChecksumException(IOExcpetion的一个子类)

<li>
由应用程序来处理这个异常，比如重试

</ul>
</ol>
   
<p>
<strong>读取时:</strong>
</p>
<ol>
<li>
client读取数据也要进行checksum的验证

<li>
重新计算checksum并和datanode上存储的checksum进行比较

<li>
datanode还存储了一个checksum的验证日志，这存储最后一次进行checksum验证的时间

<li>
client验证checksum成功后，就会更新这个日志

<li>
保留这份统计信息对检测损坏磁盘很有价值

</ol>

<p>
<strong>datanode对checksum进行定期验证:</strong>
</p>
<ol>
<li>
datanode使用 <strong>DataBlockScanner</strong>

<li>
对存储在其上的所有的Block进行定期验证

<li>
可以有效防止由于bit rot造成的物理磁盘损坏

</ol>

<p>
<strong>修复损坏的block:</strong>
</p>
<ol>
<li>
可以通过复制一个正常的block产生一个新的正常的block复本

<li>
当client检测到一个checksum  error时

<li>
在抛出异常前，client会向namenode报告出现error的block和其所在datanode

<li>
namenode并不会指挥client去修复这个error

<li>
namenode会异步的计划在另外的datanode重建一个这个block的复本，使其复本数量达到设置的水平

</ol>

<p>
<strong>关闭checksum验证:</strong>
</p>
<ul>
<li>
使用open()读取一个文件前调用FileSystem的setVerifyChecksum(false)方法

<li>
在shell中使用-get和-copyToLocal时添加-ignoreCrc选项

<ul>
<li>
可以将损坏的文件拷到本地进行检查，很有用的

</ul>
</ul>

<h3 id="toc_1.1.2">LocalFileSystem类 </h3>

<p>
Hadoop中的LocalFileSystem类负责执行客户端的文件验证
</p>
<ol>
<li>
当创建一个 <code>filename</code> 的文件时

<li>
客户端会在同一目录下同时创建一个隐藏文件 <code>.filename.crc</code>, 这种行为是透明的

<ul>
<li>
这个文件包含了对于文件 <code>filename</code> 每一个chunk的checksum

<li>
chunk的大小同样由io.bytes.per.checksum指定

<li>
这个文件还同时存储了这次验证所使用的chunk大小(作为元数据），即当前io.bytes.per.checksum的大小

<li>
因此以后即使修改了io.bytes.per.checksum的配置，也不影响恢复文件

</ul>
<li>
客户端在读取文件时，会验证Checksums, 如果发生error, LocalFileSystem会抛出一个ChecksumException

</ol>
 
<p>
<strong>禁止验证</strong> 
</p>

<p>
可以禁止验证，特别是当底层系统已经支持的情况下，这时可以 <strong>使用RawLocalFileSystem代替LocalFilesystem</strong>
</p>
<ul>
<li>
在应用全局内生效：将属性fs.file.impl设置为org.apache.hadoop.fs.RawLocalFileSysem, 它实现了对文件URI的重新映射

<li>
某次读取使用，可以生成一个RawLocalFileSystem实例
<pre  class="brush:java">
      Configuration conf = ...
      FileSystem fs = new RawLocalFileSystem();
      fs.initialize(null, conf);
</pre>

</ul>
  
<h3 id="toc_1.1.3">ChecksumFileSystem类 </h3>

<p>
LocalFileSystem使用ChecksumFileSystem去做这它的工作（晕死，有完没完)
</p>

<p>
ChecksumFileSystem很容易给其他的文件系统添加文件验证的功能，它其实是FileSystem类的一个包装类Wrapper
</p>
<pre  class="brush:java">
    FileSystem rawFs = ...
    FileSystem checksummedFs = new ChecksumFileSystem(rawFs);
</pre>
  
<ol>
<li>
rawFs是具体的底层文件系统

<li>
ChecksumFileSystem的getRawFileSystem()可以获取它

<li>
ChecksumFileSysem的getChecksumFile()可以获得任一个文件校验文件checksumfile的路径

<li>
发生错误时，会调用ChecksumFileSysem的reportCheckFailure(), 默认什么也不做

<ul>
<li>
LocalFileSystem对这个方法的实现 ：

<li>
将出错文件 和它的校验文件移到同一设备的一个side directory , 名叫bad_files

<li>
管理员应该定期检查这个文件夹

</ul>
</ol>
  
<h2 id="toc_1.2">文件压缩File Compression</h2>
<p>
<strong>文件压缩的两大好处:</strong>
</p>
<ul>
<li>
减少存储空间

<li>
加快传输

</ul>
 
<p>
<strong>常用压缩格式:</strong>
<table>
<tr>
<th>
Compression format
</th>
<th>
Tool
</th>
<th>
Algorithm
</th>
<th>
Filename extension
</th>
<th>
Splittable?
</th>
</tr>
<tr>
<td>
DEFLATE
</td>
<td>
N/A
</td>
<td>
DEFLATE
</td>
<td>
.deflate
</td>
<td>
No
</td>
</tr>
<tr>
<td>
gzip
</td>
<td>
gzip
</td>
<td>
DEFLATE
</td>
<td>
.gz
</td>
<td>
No
</td>
</tr>
<tr>
<td>
bzip2
</td>
<td>
bzip2
</td>
<td>
bzip2
</td>
<td>
.bz2
</td>
<td>
Yes
</td>
</tr>
<tr>
<td>
LZO
</td>
<td>
lzop
</td>
<td>
LZO
</td>
<td>
.lzo
</td>
<td>
No(可以预建切分索引，以支持切分） 
</td>
</tr>
<tr>
<td>
LZ4
</td>
<td>
N/A
</td>
<td>
LZ4
</td>
<td>
.lz4
</td>
<td>
No
</td>
</tr>
<tr>
<td>
Snappy
</td>
<td>
N/A
</td>
<td>
Snappy
</td>
<td>
.snappy
</td>
<td>
No
</td>
</tr>
</table>
</p>

<h3 id="toc_1.2.1">Codecs</h3>
<ul>
<li>
一个codec代表一种压缩解压算法 

<li>
一个codec是接口CompressionCodec的一个实现

</ul>
<table>
<tr>
<th>
Compression format
</th>
<th>
Hadoop CompressionCodec
</th>
</tr>
<tr>
<td>
DEFLATE
</td>
<td>
org.apache.hadoop.io.compress.DefaultCodec
</td>
</tr>
<tr>
<td>
gzip
</td>
<td>
org.apache.hadoop.io.compress.GzipCodec
</td>
</tr>
<tr>
<td>
bzip2
</td>
<td>
org.apache.hadoop.io.compress.BZip2Codec
</td>
</tr>
<tr>
<td>
LZO
</td>
<td>
com.hadoop.compression.lzo.LzopCodec
</td>
</tr>
<tr>
<td>
LZ4
</td>
<td>
org.apache.hadoop.io.compress.Lz4Codec
</td>
</tr>
<tr>
<td>
Snappy
</td>
<td>
org.apache.hadoop.io.compress.SnappyCodec
</td>
</tr>
</table>

<p>
<strong>使用CompressionCodec对流进行压缩与解压:</strong>
</p>
<ol>
<li>
createOutputStream(OutputStream outputStream) 创建了一个CompressionOutputStream

<ul>
<li>
将未压缩的数据写入它

<li>
它将压缩过的数据写入底层Stream

</ul>
<li>
createInputStream(InputStream inputStream) 创建了一个CompressionInputStream

<ul>
<li>
从它读取未压缩过的数据

<li>
底层Stream返回的实际上是压缩过的数据

</ul>
</ol>
   
<p>
<strong>使用CompressionCodecFactory推断CompressionCodec:</strong>
</p>
<ol>
<li>
一般可以通过文件扩展名推断压缩算法

<li>
CompressionCodecFactory的方法getCodec(Path), 根据扩展名获得对应的Codec

<li>
属性 <strong>io.compression.codecs:</strong>

<ul>
<li>
定义了一个列表， CompressionCodecFactory 就是从这里面查找对应的Codec

<li>
默认列出hadoop支持的所有的codec

<li>
可以向里面注册自己自定义的

</ul>
</ol>
   
<p>
<strong>原生类库Native Library:</strong>
为了性能，推荐使用原生类库进行解压和压缩。
</p>

<ol>
<li>
并不是所有的压缩方式都有对应的原生类库

<li>
如果找不到原生类库，就使用java实现 

<li>
原生类库默认在 <code>lib/native</code> 下

<li>
原生类库位置可以通过java系统的属性指定：java.library.path

<li>
默认hadoop会自动搜索自己运行所在平台的原生类库

<li>
搜到自动加载

<li>
可以通过属性haddop.native.lib禁止使用原生类库，设为false

<li>
如果使用原生库且做大量压缩与解压， 通过 <strong>CodecPool</strong> 可以重复使用 <strong>Compressor</strong> 实例，提升性能：

<ul>
<li>
就像线程池一样，有借有还

<li>
借：CodecPool.getCompressor(CompressionCodec)

<li>
还：CodecPool.returnCompressor(Compressor)
<pre  class="brush:java">
          public static void main(String[] args) throws Exception {
              String codecClassname = args[0];
              Class&lt;?&gt; codecClass = Class.forName(codecClassname);
              Configuration conf = new Configuration();
              CompressionCodec codec = (CompressionCodec)
                  CodecPool.ReflectionUtils.newInstance(codecClass, conf);
              Compressor compressor = null;
              try {
                  compressor = CodecPool.getCompressor(codec);
                  CompressionOutputStream out =
                      codec.createOutputStream(System.out, compressor);
                  IOUtils.copyBytes(System.in, out, 4096, false);
                  out.finish();
              } finally {
                  CodecPool.returnCompressor(compressor);
              }
          }
</pre>

</ul>
</ol>
      
<h3 id="toc_1.2.2">压缩与输入切片Compression and Input Splitting</h3>
<p>
当被压缩的数据要被MapReduce使用时，该压缩算法是否支持切片将变的十分重要.
</p>
<ul>
<li>
如果不支持切片，单个数据块就无法作为数据切片供 MapReduce使用

<li>
但可以将整个文件的所有数据块作为单个Map任务的输入，会失去MapReduce的优势

</ul>

<p>
<strong>使用哪种压缩格式？ </strong>
</p>
<ul>
<li>
与具体的应用有关, 要考虑文件的大小，格式，处理所用的工具

<li>
下面有一些方案，效率有高到低：

<ol>
<li>
使用Sequence File或Avro datafile. 两者都支持压缩与切片，选择一个快速算法：LZO, LZ4, Snappy

<li>
使用一种支持切片的压缩格式，比如bzip2,或者LZO(可以通过建立切分点索引以支持切片）

<li>
application将文件分成一个个的chunk，每个chunk单独压缩存储， 每个chunk压缩后的大小应该近似于HDFS的block

<li>
不压缩直接存储

</ol>
<li>
不应该大文件使用不支持切分的压缩格式，MapReduce将失去优势

</ul>

<h3 id="toc_1.2.3">在MapReduce中使用压缩Using Compression in MapReduce</h3>
<p>
<strong>输入:</strong>
</p>
<ol>
<li>
一个压缩的文件作为MapReduce的输入，会自动进行解压

<li>
使用文件扩展名来确定所使用的Codec

</ol>

<p>
<strong>输出:</strong>
</p>
<ol>
<li>
可以确定是否对MapReduce的输出进入压缩。

<li>
通过属性 <code>mapred.output.compress</code> 来指定， true or false

<li>
通过属性 <code>mapred.output.compression.codec</code> 来指定所使用的Codec

<li>
如果不通过属性指定，可以通过FileOutputFormat的静态方法来指定
<pre  class="brush:java">
    FileOutputFormat.setCompressOutput(job, true);
    FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);
</pre>

</ol>


<p>
<strong>MapReduce compression properties:</strong>
<table>
<tr>
<th>
Property name
</th>
<th>
Type
</th>
<th>
Default  value
</th>
<th>
Description
</th>
<th>
&nbsp;
</th>
</tr>
<tr>
<td>
mapred.output.compress
</td>
<td>
boolean
</td>
<td>
false
</td>
<td>
Compress outputs
</td>
<td>
&nbsp;
</td>
</tr>
<tr>
<td>
mapred.output.compression.codec
</td>
<td>
Class  name
</td>
<td>
org.apache.hadoop.io.compress.DefaultCodec
</td>
<td>
The compression codec to use for outputs
</td>
<td>
&nbsp;
</td>
</tr>
<tr>
<td>
mapred.output.compression.type
</td>
<td>
String
</td>
<td>
RECORD
</td>
<td>
The type of compression to use for SequenceFile outputs: NONE, RECORD, or BLOCK
</td>
<td>
&nbsp;
</td>
</tr>
</table>
</p>

<p>
<strong>压缩Map输出Compressing Map Output:</strong>
</p>
<ol>
<li>
对Map task产生的中间结果进行压缩也是可以获得好处的

<li>
Map的输出会存储到磁盘上，并通过网络传输到Reduce结点

<li>
如果使用快速压缩算法LZO LZ4 或Snappy,就可以获得性能提升

<li>
相关属性：

</ol>
<table>
<tr>
<th>
Property name
</th>
<th>
Type
</th>
<th>
Default value
</th>
<th>
Description
</th>
</tr>
<tr>
<td>
mapred.compress.map.output
</td>
<td>
boolean
</td>
<td>
false
</td>
<td>
Compress map outputs
</td>
</tr>
<tr>
<td>
mapred.map.output.compression.codec
</td>
<td>
Class
</td>
<td>
org.apache.hadoop.io.compress.DefaultCodec
</td>
<td>
The compression codec to use for map output
</td>
</tr>
</table>

<ul>
<li>
代码示例：
<pre  class="brush:java">
    Configuration conf = new Configuration();
    conf.setBoolean("mapred.compress.map.output", true);
    conf.setClass("mapred.map.output.compression.codec", GzipCodec.class,
            CompressionCodec.class);
    Job job = new Job(conf);
</pre>

</ul>

<h2 id="toc_1.3">序列化Serialization</h2>
<p>
<strong>序列化Serialization:</strong> 将结构化数据转化为字节流
</p>

<p>
<strong>反序列化Deserailization:</strong> 将字节流转化回结构化数据
</p>

<p>
序列化经常用于两个方面 <strong>进程间通信Interproess communication</strong> 和 <strong>永久存储Persistent storage</strong>
</p>

<p>
Hadoop使用RPC进行不同结点间的定义，RPC协议对消息进行Serialization和Deserialization
</p>

<p>
不管通信还是永久存储对序列化格式都有如下要求（但解释的侧重点不同）：
</p>
<ul>
<li>
Compact 紧凑

<li>
Fast 快速

<li>
Extensible 可扩展

<li>
Interoprable 互操作性

</ul>

<p>
Hadoop使用自己的序列化格式Writable, 紧凑，快速，但很难被其他的语言所扩展
</p>

<h3 id="toc_1.3.1">Writable接口</h3>
<p>
<strong>两个方法:</strong>
</p>
<pre  class="brush:java">
    public interface Writable {
        void write(DataOutput out) throws IOException;
        void readFields(DataInput in) throws IOException;
    }
</pre>

			</div>

			<div id="bottom">
				&copy; 2012 王兴朝
			</div>
		</div>
	<div>
</body>
</html>
