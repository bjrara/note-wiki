<!DOCTYPE html>
<html>
<head>

<title>Hadoop I/O</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">

<script type="text/javascript" src="../../js/jquery-1.6.4.min.js"></script>

<link rel="Stylesheet" type="text/css" href="../../js/sh/styles/shCore.css">
<link rel="Stylesheet" type="text/css" href="../../js/sh/styles/shThemeRDark.css">
<script type="text/javascript" src="../../js/sh/scripts/shCore.js"></script>
<script type="text/javascript" src="../../js/sh/scripts/shAutoloader.js"></script>

<script type="text/javascript" src="../../js/main.js"></script>
<script type="text/javascript" src="../../js/ASCIIMathML.js"></script>

<link rel="Stylesheet" type="text/css" href="../../style.css">
<link rel="Stylesheet" type="text/css" href="../../css/main.css">

</head>
<body>
	<div class="hidden">
		<input id="root_path" type="text" value="../../">
	</div>
	<div id="body-wrapper">
		<div id="container">
			<div id="top">
				<div id="page-title">
					<a href="../../index.html">烂笔头</a>
				</div>
				<ul id="top-nav">
				</ul>
			</div>
			<div id="middle">
				
<div class="toc">
<ul>
<li><a href="#toc_1">Hadoop I/O</a>
<ul>
<li><a href="#toc_1.1">数据完整性 Integrity</a>
<ul>
<li><a href="#toc_1.1.1">HDFS中的数据完整性 </a>
<li><a href="#toc_1.1.2">LocalFileSystem类 </a>
<li><a href="#toc_1.1.3">ChecksumFileSystem类 </a>
</ul>
<li><a href="#toc_1.2">文件压缩File Compression</a>
<ul>
<li><a href="#toc_1.2.1">Codecs</a>
</ul>
</ul>
</ul>
</div>
<h1 id="toc_1">Hadoop I/O</h1>

<h2 id="toc_1.1">数据完整性 Integrity</h2>
<p>
为了保证数据不丢失和损坏，在写入或读出数据时进行校验
</p>

<h3 id="toc_1.1.1">HDFS中的数据完整性 </h3>
<p>
<strong>校验和checksum:</strong>
</p>
<ul>
<li>
HDFS的写入数据时计算较验和，在读取时验证校验和

<li>
HDFS对每io.bytes.per.checksum个（default 512)字节计算checksum

<li>
使用CRC-32, checksum为4bytes

<li>
默认存储checksum的额外开销低于1%

</ul>
 
<p>
<strong>写入时:</strong>
</p>
<ol>
<li>
Datanode在存储它接收到的data和它们的checksum前，进行验证

<li>
这些数据来自client或其他datanode

<li>
当client向pipeline写入数据时:

<ul>
<li>
只有最后一个datanode负责验证checksum

<li>
验证出错时，client会收到一个ChecksumException(IOExcpetion的一个子类)

<li>
由应用程序来处理这个异常，比如重试

</ul>
</ol>
   
<p>
<strong>读取时:</strong>
</p>
<ol>
<li>
client读取数据也要进行checksum的验证

<li>
重新计算checksum并和datanode上存储的checksum进行比较

<li>
datanode还存储了一个checksum的验证日志，这存储最后一次进行checksum验证的时间

<li>
client验证checksum成功后，就会更新这个日志

<li>
保留这份统计信息对检测损坏磁盘很有价值

</ol>

<p>
<strong>datanode对checksum进行定期验证:</strong>
</p>
<ol>
<li>
datanode使用 <strong>DataBlockScanner</strong>

<li>
对存储在其上的所有的Block进行定期验证

<li>
可以有效防止由于bit rot造成的物理磁盘损坏

</ol>

<p>
<strong>修复损坏的block:</strong>
</p>
<ol>
<li>
可以通过复制一个正常的block产生一个新的正常的block复本

<li>
当client检测到一个checksum  error时

<li>
在抛出异常前，client会向namenode报告出现error的block和其所在datanode

<li>
namenode并不会指挥client去修复这个error

<li>
namenode会异步的计划在另外的datanode重建一个这个block的复本，使其复本数量达到设置的水平

</ol>

<p>
<strong>关闭checksum验证:</strong>
</p>
<ul>
<li>
使用open()读取一个文件前调用FileSystem的setVerifyChecksum(false)方法

<li>
在shell中使用-get和-copyToLocal时添加-ignoreCrc选项

<ul>
<li>
可以将损坏的文件拷到本地进行检查，很有用的

</ul>
</ul>

<h3 id="toc_1.1.2">LocalFileSystem类 </h3>

<p>
Hadoop中的LocalFileSystem类负责执行客户端的文件验证
</p>
<ol>
<li>
当创建一个 <code>filename</code> 的文件时

<li>
客户端会在同一目录下同时创建一个隐藏文件 <code>.filename.crc</code>, 这种行为是透明的

<ul>
<li>
这个文件包含了对于文件 <code>filename</code> 每一个chunk的checksum

<li>
chunk的大小同样由io.bytes.per.checksum指定

<li>
这个文件还同时存储了这次验证所使用的chunk大小(作为元数据），即当前io.bytes.per.checksum的大小

<li>
因此以后即使修改了io.bytes.per.checksum的配置，也不影响恢复文件

</ul>
<li>
客户端在读取文件时，会验证Checksums, 如果发生error, LocalFileSystem会抛出一个ChecksumException

</ol>
 
<p>
<strong>禁止验证</strong> 
</p>

<p>
可以禁止验证，特别是当底层系统已经支持的情况下，这时可以 <strong>使用RawLocalFileSystem代替LocalFilesystem</strong>
</p>
<ul>
<li>
在应用全局内生效：将属性fs.file.impl设置为org.apache.hadoop.fs.RawLocalFileSysem, 它实现了对文件URI的重新映射

<li>
某次读取使用，可以生成一个RawLocalFileSystem实例
<pre  class="brush:java">
      Configuration conf = ...
      FileSystem fs = new RawLocalFileSystem();
      fs.initialize(null, conf);
</pre>

</ul>
  
<h3 id="toc_1.1.3">ChecksumFileSystem类 </h3>

<p>
LocalFileSystem使用ChecksumFileSystem去做这它的工作（晕死，有完没完)
</p>

<p>
ChecksumFileSystem很容易给其他的文件系统添加文件验证的功能，它其实是FileSystem类的一个包装类Wrapper
</p>
<pre  class="brush:java">
    FileSystem rawFs = ...
    FileSystem checksummedFs = new ChecksumFileSystem(rawFs);
</pre>
  
<ol>
<li>
rawFs是具体的底层文件系统

<li>
ChecksumFileSystem的getRawFileSystem()可以获取它

<li>
ChecksumFileSysem的getChecksumFile()可以获得任一个文件校验文件checksumfile的路径

<li>
发生错误时，会调用ChecksumFileSysem的reportCheckFailure(), 默认什么也不做

<ul>
<li>
LocalFileSystem对这个方法的实现 ：

<li>
将出错文件 和它的校验文件移到同一设备的一个side directory , 名叫bad_files

<li>
管理员应该定期检查这个文件夹

</ul>
</ol>
  
<h2 id="toc_1.2">文件压缩File Compression</h2>
<p>
<strong>文件压缩的两大好处:</strong>
</p>
<ul>
<li>
减少存储空间

<li>
加快传输

</ul>

<h3 id="toc_1.2.1">Codecs</h3>
<ul>
<li>
一个codec代表一种压缩解压算法 

<li>
一个codec是接口CompressionCodec的一个实现

</ul>
<table>
<tr>
<th>
Compression format
</th>
<th>
Hadoop CompressionCodec
</th>
</tr>
<tr>
<td>
DEFLATE
</td>
<td>
org.apache.hadoop.io.compress.DefaultCodec
</td>
</tr>
<tr>
<td>
gzip
</td>
<td>
org.apache.hadoop.io.compress.GzipCodec
</td>
</tr>
<tr>
<td>
bzip2
</td>
<td>
org.apache.hadoop.io.compress.BZip2Codec
</td>
</tr>
<tr>
<td>
LZO
</td>
<td>
com.hadoop.compression.lzo.LzopCodec
</td>
</tr>
<tr>
<td>
LZ4
</td>
<td>
org.apache.hadoop.io.compress.Lz4Codec
</td>
</tr>
<tr>
<td>
Snappy
</td>
<td>
org.apache.hadoop.io.compress.SnappyCodec
</td>
</tr>
</table>

<p>
<strong>使用CompressionCodec对流进行压缩与解压:</strong>
</p>
<ol>
<li>
createOutputStream(OutputStream outputStream) 创建了一个CompressionOutputStream

<ul>
<li>
将未压缩的数据写入它

<li>
它将压缩过的数据写入底层Stream

</ul>
<li>
createInputStream(InputStream inputStream) 创建了一个CompressionInputStream

<ul>
<li>
从它读取未压缩过的数据

<li>
底层Stream返回的实际上是压缩过的数据

</ul>
</ol>
   
<p>
<strong>使用CompressionCodecFactory推断CompressionCodec:</strong>
</p>
<ol>
<li>
一般可以通过文件扩展名推断压缩算法

<li>
CompressionCodecFactory的方法getCodec(Path), 根据扩展名获得对应的Codec

</ol>

			</div>

			<div id="bottom">
				&copy; 2012 王兴朝
			</div>
		</div>
	<div>
</body>
</html>
