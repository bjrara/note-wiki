%title Hadoop I/O
%toc
=Hadoop I/O=

==数据完整性 Integrity ==
为了保证数据不丢失和损坏，在写入或读出数据时进行校验

===HDFS中的数据完整性 ===
*校验和checksum:*
* HDFS的写入数据时计算较验和，在读取时验证校验和
* HDFS对每io.bytes.per.checksum个（default 512)字节计算checksum
* 使用CRC-32, checksum为4bytes
* 默认存储checksum的额外开销低于1%
 
*写入时:*
# Datanode在存储它接收到的data和它们的checksum前，进行验证
# 这些数据来自client或其他datanode
# 当client向pipeline写入数据时:
  * 只有最后一个datanode负责验证checksum
  * 验证出错时，client会收到一个ChecksumException(IOExcpetion的一个子类)
  * 由应用程序来处理这个异常，比如重试
   
*读取时:*
# client读取数据也要进行checksum的验证
# 重新计算checksum并和datanode上存储的checksum进行比较
# datanode还存储了一个checksum的验证日志，这存储最后一次进行checksum验证的时间
# client验证checksum成功后，就会更新这个日志
# 保留这份统计信息对检测损坏磁盘很有价值

*datanode对checksum进行定期验证:*
# datanode使用 *DataBlockScanner*
# 对存储在其上的所有的Block进行定期验证
# 可以有效防止由于bit rot造成的物理磁盘损坏

*修复损坏的block:*
# 可以通过复制一个正常的block产生一个新的正常的block复本
# 当client检测到一个checksum  error时
# 在抛出异常前，client会向namenode报告出现error的block和其所在datanode
# namenode并不会指挥client去修复这个error
# namenode会异步的计划在另外的datanode重建一个这个block的复本，使其复本数量达到设置的水平

*关闭checksum验证:*
* 使用open()读取一个文件前调用FileSystem的setVerifyChecksum(false)方法
* 在shell中使用-get和-copyToLocal时添加-ignoreCrc选项
  * 可以将损坏的文件拷到本地进行检查，很有用的

===LocalFileSystem类 ===

Hadoop中的LocalFileSystem类负责执行客户端的文件验证
# 当创建一个 `filename` 的文件时
# 客户端会在同一目录下同时创建一个隐藏文件 `.filename.crc`, 这种行为是透明的
  * 这个文件包含了对于文件 `filename` 每一个chunk的checksum
  * chunk的大小同样由io.bytes.per.checksum指定
  * 这个文件还同时存储了这次验证所使用的chunk大小(作为元数据），即当前io.bytes.per.checksum的大小
  * 因此以后即使修改了io.bytes.per.checksum的配置，也不影响恢复文件
# 客户端在读取文件时，会验证Checksums, 如果发生error, LocalFileSystem会抛出一个ChecksumException
 
*禁止验证* 

可以禁止验证，特别是当底层系统已经支持的情况下，这时可以 *使用RawLocalFileSystem代替LocalFilesystem*
* 在应用全局内生效：将属性fs.file.impl设置为org.apache.hadoop.fs.RawLocalFileSysem, 它实现了对文件URI的重新映射
* 某次读取使用，可以生成一个RawLocalFileSystem实例
  {{{ class="brush:java"
      Configuration conf = ...
      FileSystem fs = new RawLocalFileSystem();
      fs.initialize(null, conf);
  }}}
  
===ChecksumFileSystem类 ===

LocalFileSystem使用ChecksumFileSystem去做这它的工作（晕死，有完没完)

ChecksumFileSystem很容易给其他的文件系统添加文件验证的功能，它其实是FileSystem类的一个包装类Wrapper
  {{{ class="brush:java"
    FileSystem rawFs = ...
    FileSystem checksummedFs = new ChecksumFileSystem(rawFs);
  }}}
  
# rawFs是具体的底层文件系统
# ChecksumFileSystem的getRawFileSystem()可以获取它
# ChecksumFileSysem的getChecksumFile()可以获得任一个文件校验文件checksumfile的路径
# 发生错误时，会调用ChecksumFileSysem的reportCheckFailure(), 默认什么也不做
  * LocalFileSystem对这个方法的实现 ：
  * 将出错文件 和它的校验文件移到同一设备的一个side directory , 名叫bad_files
  * 管理员应该定期检查这个文件夹
  
==文件压缩File Compression==
*文件压缩的两大好处:*
* 减少存储空间
* 加快传输
 
*常用压缩格式:*
| Compression format | Tool  | Algorithm | Filename extension | Splittable?                       |
|--------------------|-------|-----------|--------------------|-----------------------------------|
| DEFLATE            | N/A   | DEFLATE   | .deflate           | No                                |
| gzip               | gzip  | DEFLATE   | .gz                | No                                |
| bzip2              | bzip2 | bzip2     | .bz2               | Yes                               |
| LZO                | lzop  | LZO       | .lzo               | No(可以预建切分索引，以支持切分） |
| LZ4                | N/A   | LZ4       | .lz4               | No                                |
| Snappy             | N/A   | Snappy    | .snappy            | No                                |

===Codecs===
* 一个codec代表一种压缩解压算法 
* 一个codec是接口CompressionCodec的一个实现
| Compression format | Hadoop CompressionCodec                    |
|--------------------|--------------------------------------------|
| DEFLATE            | org.apache.hadoop.io.compress.DefaultCodec |
| gzip               | org.apache.hadoop.io.compress.GzipCodec    |
| bzip2              | org.apache.hadoop.io.compress.BZip2Codec   |
| LZO                | com.hadoop.compression.lzo.LzopCodec       |
| LZ4                | org.apache.hadoop.io.compress.Lz4Codec     |
| Snappy             | org.apache.hadoop.io.compress.SnappyCodec  |

*使用CompressionCodec对流进行压缩与解压:*
# createOutputStream(OutputStream outputStream) 创建了一个CompressionOutputStream
  * 将未压缩的数据写入它
  * 它将压缩过的数据写入底层Stream
# createInputStream(InputStream inputStream) 创建了一个CompressionInputStream
  * 从它读取未压缩过的数据
  * 底层Stream返回的实际上是压缩过的数据
   
*使用CompressionCodecFactory推断CompressionCodec:*
# 一般可以通过文件扩展名推断压缩算法
# CompressionCodecFactory的方法getCodec(Path), 根据扩展名获得对应的Codec
# 属性 *io.compression.codecs:*
  * 定义了一个列表， CompressionCodecFactory 就是从这里面查找对应的Codec
  * 默认列出hadoop支持的所有的codec
  * 可以向里面注册自己自定义的
   
*原生类库Native Library:*
为了性能，推荐使用原生类库进行解压和压缩。

# 并不是所有的压缩方式都有对应的原生类库
# 如果找不到原生类库，就使用java实现 
# 原生类库默认在 `lib/native` 下
# 原生类库位置可以通过java系统的属性指定：java.library.path
# 默认hadoop会自动搜索自己运行所在平台的原生类库
# 搜到自动加载
# 可以通过属性haddop.native.lib禁止使用原生类库，设为false
# 如果使用原生库且做大量压缩与解压， 通过 *CodecPool* 可以重复使用 *Compressor* 实例，提升性能：
  * 就像线程池一样，有借有还
  * 借：CodecPool.getCompressor(CompressionCodec)
  * 还：CodecPool.returnCompressor(Compressor)
    {{{ class="brush:java"
          public static void main(String[] args) throws Exception {
              String codecClassname = args[0];
              Class<?> codecClass = Class.forName(codecClassname);
              Configuration conf = new Configuration();
              CompressionCodec codec = (CompressionCodec)
                  CodecPool.ReflectionUtils.newInstance(codecClass, conf);
              Compressor compressor = null;
              try {
                  compressor = CodecPool.getCompressor(codec);
                  CompressionOutputStream out =
                      codec.createOutputStream(System.out, compressor);
                  IOUtils.copyBytes(System.in, out, 4096, false);
                  out.finish();
              } finally {
                  CodecPool.returnCompressor(compressor);
              }
          }
      }}}
      
===压缩与输入切片Compression and Input Splitting ===
当被压缩的数据要被MapReduce使用时，该压缩算法是否支持切片将变的十分重要.
* 如果不支持切片，单个数据块就无法作为数据切片供 MapReduce使用
* 但可以将整个文件的所有数据块作为单个Map任务的输入，会失去MapReduce的优势

*使用哪种压缩格式？ *
* 与具体的应用有关, 要考虑文件的大小，格式，处理所用的工具
* 下面有一些方案，效率有高到低：
  # 使用Sequence File或Avro datafile. 两者都支持压缩与切片，选择一个快速算法：LZO, LZ4, Snappy
  # 使用一种支持切片的压缩格式，比如bzip2,或者LZO(可以通过建立切分点索引以支持切片）
  # application将文件分成一个个的chunk，每个chunk单独压缩存储， 每个chunk压缩后的大小应该近似于HDFS的block
  # 不压缩直接存储
* 不应该大文件使用不支持切分的压缩格式，MapReduce将失去优势

===在MapReduce中使用压缩Using Compression in MapReduce===
*输入:*
# 一个压缩的文件作为MapReduce的输入，会自动进行解压
# 使用文件扩展名来确定所使用的Codec

*输出:*
# 可以确定是否对MapReduce的输出进入压缩。
# 通过属性 `mapred.output.compress` 来指定， true or false
# 通过属性 `mapred.output.compression.codec` 来指定所使用的Codec
# 如果不通过属性指定，可以通过FileOutputFormat的静态方法来指定
{{{ class="brush:java"
    FileOutputFormat.setCompressOutput(job, true);
    FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);
  }}}


*MapReduce compression properties:*
| Property name                   | Type        | Default  value                             | Description                                                                     |   |
|---------------------------------|-------------|--------------------------------------------|---------------------------------------------------------------------------------|---|
| mapred.output.compress          | boolean     | false                                      | Compress outputs                                                                |   |
| mapred.output.compression.codec | Class  name | org.apache.hadoop.io.compress.DefaultCodec | The compression codec to use for outputs                                        |   |
| mapred.output.compression.type  | String      | RECORD                                     | The type of compression to use for SequenceFile outputs: NONE, RECORD, or BLOCK |   |

*压缩Map输出Compressing Map Output:*
# 对Map task产生的中间结果进行压缩也是可以获得好处的
# Map的输出会存储到磁盘上，并通过网络传输到Reduce结点
# 如果使用快速压缩算法LZO LZ4 或Snappy,就可以获得性能提升
# 相关属性：
| Property name                       | Type    | Default value                              | Description                                 |
|-------------------------------------|---------|--------------------------------------------|---------------------------------------------|
| mapred.compress.map.output          | boolean | false                                      | Compress map outputs                        |
| mapred.map.output.compression.codec | Class   | org.apache.hadoop.io.compress.DefaultCodec | The compression codec to use for map output |

* 代码示例：
{{{ class="brush:java"
    Configuration conf = new Configuration();
    conf.setBoolean("mapred.compress.map.output", true);
    conf.setClass("mapred.map.output.compression.codec", GzipCodec.class,
            CompressionCodec.class);
    Job job = new Job(conf);
  }}}

==序列化Serialization==
*序列化Serialization:* 将结构化数据转化为字节流

*反序列化Deserailization:* 将字节流转化回结构化数据

序列化经常用于两个方面 *进程间通信Interproess communication* 和 *永久存储Persistent storage*

Hadoop使用RPC进行不同结点间的定义，RPC协议对消息进行Serialization和Deserialization

不管通信还是永久存储对序列化格式都有如下要求（但解释的侧重点不同）：
* Compact 紧凑
* Fast 快速
* Extensible 可扩展
* Interoprable 互操作性

Hadoop使用自己的序列化格式Writable, 紧凑，快速，但很难被其他的语言所扩展

===Writable接口 ===
*两个方法:*
{{{ class="brush:java"
    public interface Writable {
        void write(DataOutput out) throws IOException;
        void readFields(DataInput in) throws IOException;
    }
  }}}




